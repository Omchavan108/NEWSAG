import logging
from fastapi import APIRouter, HTTPException
from app.services.news_service import GNewsService
from app.services.sentiment_ml import SentimentService  # ✅ Use new ML-based sentiment
from app.core.cache import get_from_cache, set_in_cache, delete_from_cache
from app.core.gnews_counter import GNewsCounter

router = APIRouter()
logger = logging.getLogger(__name__)

# Default categories
CATEGORIES = ["general", "nation", "business", "technology", "sports", "entertainment", "health"]


# -----------------------------
# SEARCH SUGGESTIONS (CACHE ONLY)
# -----------------------------
@router.get("/suggestions")
async def get_search_suggestions(q: str):
    """
    Return matching articles from Redis-cached categories only.
    Matches query against title, description, content, and source (case-insensitive).
    """
    query = (q or "").strip()
    if len(query) < 2:
        logger.info("[SUGGESTIONS REJECTED] query too short")
        raise HTTPException(status_code=400, detail="Query must be at least 2 characters")

    query_lower = query.lower()
    results = []
    seen_ids = set()

    for category in CATEGORIES:
        cache_key = f"gnews:{category}"
        cached = await get_from_cache(cache_key)
        if not cached:
            continue

        for article in cached:
            title = (article.get("title") or "").lower()
            description = (article.get("description") or "").lower()
            content = (article.get("content") or "").lower()
            source = article.get("source") or ""
            if isinstance(source, dict):
                source = source.get("name") or ""
            source_lower = source.lower()

            if query_lower not in title and query_lower not in description and query_lower not in content and query_lower not in source_lower:
                continue

            article_id = article.get("id") or article.get("url")
            if article_id in seen_ids:
                continue
            seen_ids.add(article_id)

            article_out = dict(article)
            if isinstance(article_out.get("source"), dict):
                article_out["source"] = article_out.get("source", {}).get("name") or ""

            results.append(article_out)

            if len(results) >= 6:
                break

        if len(results) >= 6:
            break

    logger.info(f"[SUGGESTIONS] query='{query_lower}' | count={len(results)}")

    return {
        "query": query,
        "count": len(results),
        "articles": results[:6],
    }


# -----------------------------
# GET TRENDING HEADLINES (BULLETIN)
# -----------------------------
@router.get("/trending/headlines")
async def get_trending_headlines(max_items: int = 8):
    """
    Fetch trending top headlines for the bulletin ticker.
    Uses cached general news or fetches fresh if needed.
    Returns lightweight headline data for the ticker display.
    """
    cache_key = "gnews:trending:headlines"
    
    # Try trending headlines cache first
    cached = await get_from_cache(cache_key)
    if cached:
        logger.info(f"[CACHE HIT] trending headlines | count={len(cached)}")
        hit_status = await GNewsCounter.get_hit_status()
        return {
            "source": "cache",
            "count": len(cached),
            "headlines": cached,
            "hits": hit_status,
        }
    
    # Fallback: Use general news cache to avoid extra API hit
    logger.info("[CACHE MISS] trending headlines | checking general news cache...")
    general_cache = await get_from_cache("gnews:general")
    
    if general_cache:
        logger.info(f"[CACHE HIT] general news for trending | extracting {max_items} headlines")
        # Extract headlines from cached general news
        headlines = [
            {
                "id": article.get("id"),
                "title": article.get("title"),
                "source": article.get("source"),
                "url": article.get("url"),
                "published_at": article.get("published_at"),
                "category": article.get("category", "general"),
            }
            for article in general_cache[:max_items]
        ]
        # Cache the extracted headlines with shorter TTL
        await set_in_cache(cache_key, headlines, ttl=60 * 10)  # 10 min TTL
        logger.info(f"[CACHE SET] trending headlines | count={len(headlines)} | ttl=600s")
        hit_status = await GNewsCounter.get_hit_status()
        return {
            "source": "cache",
            "count": len(headlines),
            "headlines": headlines,
            "hits": hit_status,
        }
    
    # No cache available - fetch fresh general news (uses 1 API hit)
    logger.warning("[GNEWS HIT] trending headlines | no cache available, fetching fresh...")
    try:
        articles = await GNewsService.fetch_category("general")
        logger.info(f"[GNEWS OK] trending headlines | fetched {len(articles)} articles")
    except Exception as e:
        logger.error(f"[GNEWS ERROR] trending headlines | {str(e)}")
        raise HTTPException(status_code=502, detail=str(e))
    
    # Extract headlines (no sentiment needed for ticker - faster response)
    headlines = [
        {
            "id": article.get("id"),
            "title": article.get("title"),
            "source": article.get("source"),
            "url": article.get("url"),
            "published_at": article.get("published_at"),
            "category": article.get("category", "general"),
        }
        for article in articles[:max_items]
    ]
    
    # Cache trending headlines
    await set_in_cache(cache_key, headlines, ttl=60 * 10)  # 10 min TTL
    logger.info(f"[CACHE SET] trending headlines | count={len(headlines)} | ttl=600s")
    
    # Also cache full articles for general category (avoids double fetch)
    await set_in_cache("gnews:general", articles)
    logger.info(f"[CACHE SET] general news (from trending) | count={len(articles)}")
    
    hit_status = await GNewsCounter.get_hit_status()
    
    return {
        "source": "api",
        "count": len(headlines),
        "headlines": headlines,
        "hits": hit_status,
    }

# --------------------------------------------------
# HELPER: Add ML-based sentiment to articles
# --------------------------------------------------
async def add_sentiment_to_articles(articles):
    """
    Calculate sentiment for each article using ML model.
    Combines title + description + content for analysis.
    Includes Redis caching to avoid repeated ML inference.
    """
    for article in articles:
        # Use ML service to analyze article sentiment (checks Redis cache first)
        sentiment_result = await SentimentService.analyze_article(
            title=article.get('title', ''),
            description=article.get('description', ''),
            content=article.get('content', '')
        )
        
        # Attach sentiment to article
        article["sentiment"] = {
            "label": sentiment_result["label"],
            "confidence": sentiment_result["confidence"],
            "model": sentiment_result["model"]
        }
    
    return articles

# -----------------------------
# GET NEWS BY TOPIC (CACHE FIRST)
# -----------------------------
@router.get("/topic/{topic}")
async def get_news_by_topic(topic: str):
    """Fetch news by topic/category with caching"""
    # TODO: Future enhancement - include country/language/pagination in cache key
    cache_key = f"gnews:{topic}"

    cached = await get_from_cache(cache_key)
    if cached:
        logger.info(f"[CACHE HIT] {topic}")
        # Cached articles ALREADY have sentiment - do NOT recompute
        # (Sentiment was added before caching, see API fetch branch below)
        hit_status = await GNewsCounter.get_hit_status()
        return {
            "source": "cache",
            "count": len(cached),
            "articles": cached,
            "hits": hit_status,
        }

    logger.info(f"[GNEWS HIT] {topic}")
    try:
        articles = await GNewsService.fetch_category(topic)
    except Exception as e:
        logger.error(f"Error fetching news for {topic}: {str(e)}")
        raise HTTPException(status_code=502, detail=str(e))

    # Add sentiment ONCE before caching (includes per-article Redis caching)
    articles = await add_sentiment_to_articles(articles)
    
    await set_in_cache(cache_key, articles)
    
    # ✅ Get hit status after API call
    hit_status = await GNewsCounter.get_hit_status()

    return {
        "source": "api",
        "count": len(articles),
        "articles": articles,
        "hits": hit_status,  # ✅ Added
    }

# Backward compatibility
@router.get("/{category}")
async def get_news(category: str):
    """Fetch news by category (backward compatibility)"""
    return await get_news_by_topic(category)

# ✅ NEW: Get hit counter status
@router.get("/status/hits")
async def get_hit_status():
    """Get current GNews API hit count for today"""
    hit_status = await GNewsCounter.get_hit_status()
    return {
        "status": "ok",
        "hits": hit_status,
        "message": "GNews API hit counter"
    }

# ✅ NEW: Admin endpoint - Reset counter (testing only)
@router.post("/admin/reset-hits")
async def reset_hit_counter():
    """Reset hit counter (ADMIN ONLY - for testing)"""
    result = await GNewsCounter.reset_counter()
    return {
        "status": "reset",
        "hits": result,
        "message": "Hit counter reset to 0"
    }


# -----------------------------
# MANUAL REFRESH (1 HIT)
# -----------------------------
@router.post("/refresh/{category}")
async def refresh_category(category: str):
    """Manually refresh news for a specific category"""
    cache_key = f"gnews:{category}"
    await delete_from_cache(cache_key)

    logger.warning(f"[MANUAL REFRESH] {category}")
    try:
        articles = await GNewsService.fetch_category(category)
    except Exception as e:
        logger.error(f"Error refreshing {category}: {str(e)}")
        raise HTTPException(status_code=502, detail=str(e))
    
    # Add sentiment BEFORE caching (computed once, cached with articles)
    articles = await add_sentiment_to_articles(articles)
    
    await set_in_cache(cache_key, articles)

    return {
        "message": f"{category} refreshed",
        "hits_used": 1,
        "articles": len(articles),
    }


# -----------------------------
# REFRESH ALL (7 HITS)
# -----------------------------
@router.post("/refresh-all")
async def refresh_all():
    """Refresh all categories at once"""
    categories = CATEGORIES
    total_articles = 0
    errors = []

    for cat in categories:
        try:
            await delete_from_cache(f"gnews:{cat}")
            articles = await GNewsService.fetch_category(cat)
            # Add sentiment BEFORE caching (computed once, cached with articles)
            articles = await add_sentiment_to_articles(articles)
            await set_in_cache(f"gnews:{cat}", articles)
            total_articles += len(articles)
        except Exception as e:
            logger.error(f"Error refreshing {cat}: {str(e)}")
            errors.append(f"{cat}: {str(e)}")

    logger.warning(f"[MANUAL REFRESH ALL] categories={len(categories)}, articles={total_articles}")

    return {
        "message": "All categories refreshed",
        "categories_refreshed": len(categories),
        "total_articles": total_articles,
        "errors": errors if errors else None,
    }
